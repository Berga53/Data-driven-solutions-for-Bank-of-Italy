{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616fe1e-0b36-4c79-ada1-86445f5d55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, Flatten, BatchNormalization, LSTM\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, fbeta_score, accuracy_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "names = ['Attività e Passività', 'Costi e Ricavi', 'Entrate', 'Spese']\n",
    "years = np.arange(2008, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb33a6-44e8-4aec-a028-449125d3eee3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd098459-3d41-4538-b884-b45c844ebbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(dataframes, comuni, pop, category, features, lag, pred, mode, add = None):\n",
    "\tyears = sorted(set(int(col.split('_')[1]) for col in dataframes.keys()))\n",
    "\tyear_ranges = [np.arange(year, year + lag) for year in np.arange(years[0], years[-1] - lag + 2)]\n",
    "\n",
    "\tdata = []\n",
    "\ty = []\n",
    "\n",
    "\tfor year_range in year_ranges:\n",
    "\t\tdata_temp = []\n",
    "\t\tfor year in year_range:\n",
    "\t\t\tpop_temp = pop[f'pop_{year}'].values[:, np.newaxis]\n",
    "\t\t\tif mode == 'pop':\n",
    "\t\t\t\ttemp = pd.concat([dataframes[f'{feature}_{year}_{category}'] for feature in features], axis=1) / pop_temp\n",
    "\t\t\telif mode == 'abs':\n",
    "\t\t\t\ttemp = pd.concat([dataframes[f'{feature}_{year}_{category}']\n",
    "\t\t\t\t                  .div(dataframes[f'{feature}_{year}_{category}'].sum(axis=1), axis=0).fillna(0)\n",
    "\t\t\t\t                  for feature in features], axis=1)\n",
    "\t\t\tif add is not None:\n",
    "\t\t\t\tfor elem in add:\n",
    "\t\t\t\t\tif elem == 'pop':\n",
    "\t\t\t\t\t\tcol = np.log(pop[f'pop_{year}'].copy())\n",
    "\t\t\t\t\t\ttemp = pd.concat([temp, col], axis = 1)\n",
    "\t\t\t\t\tif elem == 'zone':\n",
    "\t\t\t\t\t\tfor elem in zone.columns:\n",
    "\t\t\t\t\t\t\tcol = zone[elem].copy()\n",
    "\t\t\t\t\t\t\ttemp = pd.concat([temp, col], axis = 1)\n",
    "\t\t\tdata_temp.append(temp)\n",
    "\n",
    "\t\ttemp = np.stack([elem.values for elem in data_temp])\n",
    "\t\ttemp = np.swapaxes(temp, 0, 1)\n",
    "\t\tdata.append(temp)\n",
    "\t\ttemp_y = sum([comuni[f'Target {year_range[-1] + i + 1}'].values for i in range(pred)])\n",
    "\t\ty.append((temp_y > 0).astype(int))\n",
    "\n",
    "\tX = np.concatenate(data)\n",
    "\ty = np.concatenate(y)\n",
    "\tprint(X.shape)\n",
    "\t\t\t\t\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4488d6-d0b3-4cc3-868d-7e02f1db463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(dataframes, comuni, pop, category, features, lag, pred, mode, epochs, batch_size, verbose, add):\n",
    "\tX, y = create_data(dataframes, comuni, pop, category, features, lag, pred, mode, add)\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\t\n",
    "\tclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "\tclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\tclass_sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "\tcnn_model = train_model(X_train, y_train, X_test, y_test, epochs, batch_size, class_weight_dict, verbose)\n",
    "\tcnn_metrics, cnn_predictions_test, cnn_predictions_train = evaluate_model(cnn_model, X_train, y_train, X_test, y_test, verbose)\n",
    "\t\n",
    "\tX_reshaped = X.reshape(X.shape[0], -1)[:,:-lag+1]\n",
    "\tX_train_reshaped, X_test_reshaped = train_test_split(X_reshaped, test_size=0.2, stratify=y, random_state=42)\n",
    "\t\n",
    "\tmodels = {\n",
    "\t\t\"Logistic Regression\": LogisticRegression(penalty = 'l2', C = 5, solver='lbfgs', class_weight='balanced',max_iter = 100000, random_state=42),\n",
    "\t\t\"Decision Tree\": DecisionTreeClassifier(max_depth=None, random_state=42, class_weight='balanced'),\n",
    "\t\t\"Random Forest\": RandomForestClassifier(n_estimators=500, random_state=42, class_weight='balanced'),\n",
    "\t\t\"XGBoost\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\t}\n",
    "\t\n",
    "\tmodel_objects = {\"CNN\": cnn_model}\n",
    "\tresults = {\"CNN\": cnn_metrics}\n",
    "\t\n",
    "\tfor model_name, model in models.items():\n",
    "\t\tif model_name == \"Logistic Regression\":\n",
    "\t\t\tmodel.set_params(class_weight='balanced')  # Direct class weight support\n",
    "\t\telif model_name in [\"Decision Tree\", \"Random Forest\"]:\n",
    "\t\t\tmodel.set_params(class_weight=class_weight_dict)  # Pass computed class weights\n",
    "\t\tmodel.fit(X_train_reshaped, y_train, sample_weight=class_sample_weight if model_name not in [\"Logistic Regression\"] else None)\n",
    "\t\tmodel_objects[model_name] = model\n",
    "\t\t\n",
    "\t\tpredictions_test = model.predict(X_test_reshaped)\n",
    "\t\tpredictions_train = model.predict(X_train_reshaped)\n",
    "\t\tprobabilities_test = model.predict_proba(X_test_reshaped)[:, 1] if hasattr(model, \"predict_proba\") else predictions_test\n",
    "\t\tprobabilities_train = model.predict_proba(X_train_reshaped)[:, 1] if hasattr(model, \"predict_proba\") else predictions_train\n",
    "\t\t\n",
    "\t\troc_auc_test = roc_auc_score(y_test, probabilities_test)\n",
    "\t\troc_auc_train = roc_auc_score(y_train, probabilities_train)\n",
    "\t\tfpr_test, tpr_test, _ = roc_curve(y_test, probabilities_test)\n",
    "\t\tfpr_train, tpr_train, _ = roc_curve(y_train, probabilities_train)\n",
    "\t\tprecision_test, recall_test, _ = precision_recall_curve(y_test, probabilities_test)\n",
    "\t\tpr_auc_test = auc(recall_test, precision_test)\n",
    "\t\tprecision_train, recall_train, _ = precision_recall_curve(y_train, probabilities_train)\n",
    "\t\tpr_auc_train = auc(recall_train, precision_train)\n",
    "\t\t\n",
    "\t\tmetrics = {\n",
    "\t\t\t'precision_test': precision_score(y_test, predictions_test),\n",
    "\t\t\t'recall_test': recall_score(y_test, predictions_test),\n",
    "\t\t\t'f1_test': f1_score(y_test, predictions_test),\n",
    "\t\t\t'confusion_matrix_test': confusion_matrix(y_test, predictions_test),\n",
    "\t\t\t'precision_train': precision_score(y_train, predictions_train),\n",
    "\t\t\t'recall_train': recall_score(y_train, predictions_train),\n",
    "\t\t\t'f1_train': f1_score(y_train, predictions_train),\n",
    "\t\t\t'confusion_matrix_train': confusion_matrix(y_train, predictions_train),\n",
    "\t\t\t'roc_auc_test': roc_auc_test,\n",
    "\t\t\t'roc_auc_train': roc_auc_train,\n",
    "\t\t\t'roc_curve_test': (fpr_test, tpr_test),\n",
    "\t\t\t'roc_curve_train': (fpr_train, tpr_train),\n",
    "\t\t\t'pr_auc_test': pr_auc_test,\n",
    "\t\t\t'pr_auc_train': pr_auc_train,\n",
    "\t\t\t'pr_curve_test': (precision_test, recall_test),\n",
    "\t\t\t'pr_curve_train': (precision_train, recall_train)\n",
    "\t\t}\n",
    "\t\tresults[model_name] = metrics\n",
    "\t\n",
    "\treturn model_objects, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6bd7f-23d3-4b5c-b852-a250590417a3",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35cfa1-88ab-4539-ac8f-778ce740a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Denominazione', 'Dizione_Provincia', 'Dizione_Regione', 'Dizione_zona']\n",
    "\n",
    "final = pd.read_csv('Data//Anagrafe_comuni.csv', sep = ';', encoding = 'latin_1', low_memory = False).set_index('Id_Ente')\n",
    "\n",
    "final['Data_Istituzione'] = pd.to_datetime(final['Data_Istituzione'], format='%Y-%m-%d')\n",
    "final = final[final['Data_Istituzione'].dt.year < 2008]\n",
    "\n",
    "final = final[final['Data_Cessazione'].isna()]\n",
    "\n",
    "final = final[final['Codice_Tipologia_DLGS_118_2011'] == 'ELCOMU'][columns]\n",
    "\n",
    "final = final.rename(columns={'Denominazione': 'Comune', 'Dizione_Provincia': 'Provincia', 'Dizione_Regione': 'Regione', 'Dizione_zona': 'Zona'})\n",
    "final.index.names = ['BDAP']\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12086d2d-a638-4790-8c5c-9887298ec373",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Attività e Passività', 'Costi e Ricavi', 'Entrate', 'Spese']\n",
    "years = np.arange(2008, 2016)\n",
    "dataframes = {}\n",
    "\n",
    "for name in names:\n",
    "\t\n",
    "    for year in years:\n",
    "\t\t\n",
    "        for category in [\"CAT I\", \"CAT II\"]:\n",
    "\t\t\t\n",
    "            file_path = f'dati final//{name} {year} {category}.pkl'\n",
    "            df = pd.read_pickle(file_path)  \n",
    "            dataframes[f'{name}_{year}_{category}'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c71b1-341c-4690-b73f-5a114595769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comuni = pd.read_pickle('data//comuni.pkl')\n",
    "zone = pd.read_pickle('data//zona.pkl')\n",
    "pop = pd.read_pickle('data//pop.pkl')\n",
    "critici = pd.read_csv('data//CriticitàComuni.csv', sep = ';', low_memory = False)\n",
    "for year in np.arange(2008, 2020):\n",
    "\tcomuni[f'Target {year}'] = 0\n",
    "for year in np.arange(2008, 2020):\n",
    "\tfor idx, row in critici.iterrows():\n",
    "\t\tif int(row['Anno']) == year:\n",
    "\t\t\tcomune = row['Comune']\n",
    "\t\t\tcomune_2 = comune.replace('-', ' ')\n",
    "\t\t\tcomuni.loc[(comuni['Comune'] == f'COMUNE DI {comune}') | (comuni['Comune'] == f'COMUNE DI {comune_2}'), f'Target {year}'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe2702-6450-4c5d-bec6-92661f8b36f9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c5a24-86ad-4e09-9996-1d5387209110",
   "metadata": {},
   "source": [
    "## CAT I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da528328-952b-4413-8de8-410627073dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'CAT I'\n",
    "features = names[2:4]\n",
    "epochs = 500\n",
    "batch_size = 1024\n",
    "verbose = 0\n",
    "mode = 'pop'\n",
    "results = {}\n",
    "models = {}\n",
    "add = ['pop', 'zone']\n",
    "\n",
    "for pred in range(1,4):\n",
    "\tprint(f'#####PRED: {pred}#####')\n",
    "\ttemp = {}\n",
    "\tmodel = {}\n",
    "\tfor lag in range(5,9):\n",
    "\t\tmodel[lag], temp[lag] = run_training(dataframes, comuni, pop, category, features, lag, pred, mode, epochs, batch_size, verbose, add)\n",
    "\t\tprint(lag)\n",
    "\tresults[pred] = temp\n",
    "\tmodels[pred] = model\n",
    "\t\n",
    "if save:\t\n",
    "\twith open('results_CAT I_final.pickle', 'wb') as handle:\n",
    "\t    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\twith open('models_CAT I_final.pickle', 'wb') as handle:\n",
    "\t    pickle.dump(models, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27acbb8b-6518-4b1a-a4e2-4f955a738e5e",
   "metadata": {},
   "source": [
    "## CAT II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89438b8-aa61-40c6-a522-80f765b36727",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'CAT II'\n",
    "features = names[2:4]\n",
    "epochs = 500\n",
    "batch_size = 1024\n",
    "verbose = 0\n",
    "mode = 'pop'\n",
    "results = {}\n",
    "models = {}\n",
    "add = ['pop', 'zone']\n",
    "\n",
    "for pred in range(1,4):\n",
    "\tprint(f'#####PRED: {pred}#####')\n",
    "\ttemp = {}\n",
    "\tmodel = {}\n",
    "\tfor lag in range(5,9):\n",
    "\t\tmodel[lag], temp[lag] = run_training(dataframes, comuni, pop, category, features, lag, pred, mode, epochs, batch_size, verbose, add)\n",
    "\t\tprint(lag)\n",
    "\tresults[pred] = temp\n",
    "\tmodels[pred] = model\n",
    "\t\n",
    "if save:\t\n",
    "\twith open('results_CAT II_final.pickle', 'wb') as handle:\n",
    "\t    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\twith open('models_CAT II_final.pickle', 'wb') as handle:\n",
    "\t    pickle.dump(models, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
